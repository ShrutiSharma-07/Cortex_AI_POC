import streamlit as st # Import python packages
from snowflake.snowpark.context import get_active_session

from snowflake.cortex import Complete
from snowflake.core import Root

import pandas as pd
import json

pd.set_option("max_colwidth",None)

### Default Values
NUM_CHUNKS = 3 # Num-chunks provided as context. Play with this to check how it affects your accuracy
slide_window = 7 # how many last conversations to remember. This is the slide window.

# service parameters
CORTEX_SEARCH_DATABASE = "POC_POLICY"
CORTEX_SEARCH_SCHEMA = "PROCUREMENT_POLICY"
CORTEX_SEARCH_SERVICE = "POLICY_SEARCH_SERVICE"


# columns to query in the service
COLUMNS = [
    "chunk",
    "chunk_index",
    "relative_path",
    "category"
]

session = get_active_session()
root = Root(session)                         

svc = root.databases[CORTEX_SEARCH_DATABASE].schemas[CORTEX_SEARCH_SCHEMA].cortex_search_services[CORTEX_SEARCH_SERVICE]
   
### Functions
     
def config_options():
    st.sidebar.title("**Chat Configuration**")

    st.sidebar.selectbox('**Select LLM Model**',('mistral-large2', 'llama3.1-70b',
                                    'llama3.1-8b', 'snowflake-arctic'), key="model_name")

    categories = session.table('policy_docs_chunks').select('category').distinct().collect()

    cat_list = ['ALL']
    for cat in categories:
        cat_list.append(cat.CATEGORY)
            
    st.sidebar.selectbox('**Select document category**', cat_list, key = "category_value")
   
    # Add horizontal line separator
    st.sidebar.markdown("---")

    st.sidebar.title("**Chat History**")

    st.sidebar.checkbox('Remember chat history', key="use_chat_history", value = True)

    st.sidebar.checkbox('Summary of previous chat', key="debug", value = True)
    
    # Show previous conversation summary right under the checkbox
    if st.session_state.debug:
        # Check if messages exist and have enough entries before accessing
        if hasattr(st.session_state, 'messages') and st.session_state.messages and len(st.session_state.messages) >= 2:
            # Get the second-to-last message (previous question)
            previous_question = st.session_state.messages[-2]['content']
            
            # Generate summary if not already stored or if it's different
            if not hasattr(st.session_state, 'previous_question_summary') or st.session_state.previous_question_summary is None:
                summary_prompt = f"""
                    Summarize this question in a concise way (1-2 sentences):
                    {previous_question}
                    """
                try:
                    summary = Complete(st.session_state.model_name, summary_prompt).replace("'", "")
                    # Combine original question with summary
                    st.session_state.previous_question_summary = f"Question: {previous_question}\n\nSummary: {summary}"
                except:
                    st.session_state.previous_question_summary = f"Question: {previous_question}"  # Fallback to just the question
            
            st.sidebar.text("Previous Chat Summary:")
            st.sidebar.caption(st.session_state.previous_question_summary)
            
            # Show previous answer's source documents if available
            if hasattr(st.session_state, 'previous_relative_paths') and st.session_state.previous_relative_paths:
                st.sidebar.text("Source documents for previous chat:")
                for path in st.session_state.previous_relative_paths:
                    try:
                        cmd2 = f"select GET_PRESIGNED_URL(@policy_documents, '{path}', 360) as URL_LINK from directory(@policy_documents)"
                        df_url_link = session.sql(cmd2).to_pandas()
                        url_link = df_url_link._get_value(0,'URL_LINK')
                        
                        doc_name = path.split('/')[-1]
                        st.sidebar.markdown(f"ðŸ“„ [{doc_name}]({url_link})")
                        
                    except Exception as e:
                        st.sidebar.caption(f"Error loading {path}")
    
    st.sidebar.button("Start Over", key="clear_conversation", on_click=init_messages, type="primary")
    
    # Add horizontal line separator
    st.sidebar.markdown("---")
    
    # st.sidebar.expander("Session State").write(st.session_state)

def init_messages():

    # Initialize chat history
    if st.session_state.clear_conversation or "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.last_relative_paths = None
        st.session_state.last_chunks_data = None
        st.session_state.previous_relative_paths = None
        st.session_state.previous_question_summary = None

def get_similar_chunks_search_service(query):

    if st.session_state.category_value == "ALL":
        response = svc.search(query, COLUMNS, limit=NUM_CHUNKS)
    else: 
        filter_obj = {"@eq": {"category": st.session_state.category_value} }
        response = svc.search(query, COLUMNS, filter=filter_obj, limit=NUM_CHUNKS)

    # REMOVED: st.sidebar.json(response.json()) - this was displaying the huge JSON
    
    return response.json()  

def get_chat_history():
#Get the history from the st.session_stage.messages according to the slide window parameter
    
    chat_history = []
    
    start_index = max(0, len(st.session_state.messages) - slide_window)
    for i in range (start_index , len(st.session_state.messages) -1):
         chat_history.append(st.session_state.messages[i])

    return chat_history

def summarize_question_with_history(chat_history, question):
# To get the right context, use the LLM to first summarize the previous conversation
# This will be used to get embeddings and find similar chunks in the docs for context

    prompt = f"""
        Based on the chat history below and the question, generate a query that extends the question
        with the chat history provided. The query should be in natural language. 
        Answer with only the query. Do not add any explanation.
        
        <chat_history>
        {chat_history}
        </chat_history>
        <question>
        {question}
        </question>
        """
    
    summary = Complete(st.session_state.model_name, prompt)   
    summary = summary.replace("'", "")

    return summary

def create_prompt (myquestion):

    if st.session_state.use_chat_history:
        chat_history = get_chat_history()

        if chat_history != []: #There is chat_history, so not first question
            question_summary = summarize_question_with_history(chat_history, myquestion)
            prompt_context =  get_similar_chunks_search_service(question_summary)
        else:
            prompt_context = get_similar_chunks_search_service(myquestion) #First question when using history
    else:
        prompt_context = get_similar_chunks_search_service(myquestion)
        chat_history = ""
  
    prompt = f"""
           You are an expert chat assistance that extracts information from the CONTEXT provided
           between <context> and </context> tags.
           You offer a chat experience considering the information included in the CHAT HISTORY
           provided between <chat_history> and </chat_history> tags..
           When answering the question contained between <question> and </question> tags
           be concise and do not hallucinate. 
           If you donÂ´t have the information just say so.
           
           Do not mention the CONTEXT used in your answer.
           Do not mention the CHAT HISTORY used in your asnwer.

           Only answer the question if you can extract it from the CONTEXT provided.
           
           <chat_history>
           {chat_history}
           </chat_history>
           <context>          
           {prompt_context}
           </context>
           <question>  
           {myquestion}
           </question>
           Answer: 
           """
    
    json_data = json.loads(prompt_context)

    relative_paths = set(item['relative_path'] for item in json_data['results'])

    return prompt, relative_paths, json_data['results']


def answer_question(myquestion):

    prompt, relative_paths, chunks_data =create_prompt (myquestion)

    response = Complete(st.session_state.model_name, prompt)   

    return response, relative_paths, chunks_data

def show_chunks_content(chunks_data):
    """Display chunks content in sidebar"""
    with st.sidebar.expander("Source Content", expanded=True):
        for i, chunk in enumerate(chunks_data):
            st.markdown(f"**Chunk {i+1}**")
            st.markdown(f"**Document** - {chunk['relative_path']}")
            st.markdown(f"*Category: {chunk.get('category', 'N/A')}*")
            st.text_area(f"Content {i+1}:", chunk['chunk'], height=100, key=f"chunk_{i}")
            st.markdown("---")

def main():

    # Initialize session state FIRST before anything else
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.last_relative_paths = None
        st.session_state.last_chunks_data = None
        st.session_state.previous_relative_paths = None
        st.session_state.previous_question_summary = None

    # Custom CSS for styling sidebar buttons - white by default, light blue on hover
    st.markdown("""
    <style>
    .css-1d391kg .stButton > button {
        background-color: white !important;
        color: #262730 !important;
        border: 1px solid #d0d0d0 !important;
    }
    .css-1d391kg .stButton > button:hover {
        background-color: #a4defc !important;
        color: #262730 !important;
        border: 1px solid #a4defc !important;
    }
    /* Alternative selector for newer Streamlit versions */
    section[data-testid="stSidebar"] .stButton > button {
        background-color: white !important;
        color: #262730 !important;
        border: 1px solid #d0d0d0 !important;
    }
    section[data-testid="stSidebar"] .stButton > button:hover {
        background-color: #a4defc !important;
        color: #262730 !important;
        border: 1px solid #a4defc !important;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.title(f"Policy Data Chat Assistant")
    st.write("List of documents provided in context")
    docs_available = session.sql("ls @policy_documents").collect()
    list_docs = []
    for doc in docs_available:
        list_docs.append(doc["name"])
    
    # Create a clean dataframe without the "value" column name
    df_docs = pd.DataFrame(list_docs, columns=["Document Name"])
    st.dataframe(df_docs, use_container_width=True, hide_index=True)

    config_options()
    init_messages()
     
    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Accept user input
    if question := st.chat_input("What do you want to know from the policy documents?"):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": question})
        # Display user message in chat message container
        with st.chat_message("user"):
            st.markdown(question)
        # Display assistant response in chat message container
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
    
            question = question.replace("'","")
    
            with st.spinner(f"{st.session_state.model_name} thinking..."):
                response, relative_paths, chunks_data = answer_question(question)            
                response = response.replace("'", "")
                message_placeholder.markdown(response)

                # Store previous answer's data before updating with current
                if hasattr(st.session_state, 'last_relative_paths') and st.session_state.last_relative_paths:
                    st.session_state.previous_relative_paths = st.session_state.last_relative_paths
                
                # Clear previous question summary so it gets regenerated next time
                st.session_state.previous_question_summary = None
                
                # Store data in session state for persistent buttons
                st.session_state.last_relative_paths = relative_paths
                st.session_state.last_chunks_data = chunks_data

        
        st.session_state.messages.append({"role": "assistant", "content": response})
    
    # Show direct download links for current chat documents
    if hasattr(st.session_state, 'last_relative_paths') and st.session_state.last_relative_paths:
        st.sidebar.title("**Context Documentation**")
        st.sidebar.markdown("(Current Chat)")
        for path in st.session_state.last_relative_paths:
            try:
                cmd2 = f"select GET_PRESIGNED_URL(@policy_documents, '{path}', 360) as URL_LINK from directory(@policy_documents)"
                df_url_link = session.sql(cmd2).to_pandas()
                url_link = df_url_link._get_value(0,'URL_LINK')
                
                doc_name = path.split('/')[-1]
                st.sidebar.markdown(f"ðŸ“„ [{doc_name}]({url_link})")
                
            except Exception as e:
                st.sidebar.caption(f"Error loading {path}")
    
    # Keep the Show Source Data as a button
    if hasattr(st.session_state, 'last_chunks_data') and st.session_state.last_chunks_data:
        if st.sidebar.button("Show Source Data", key="show_chunks"):
            show_chunks_content(st.session_state.last_chunks_data)


if __name__ == "__main__":
    main()